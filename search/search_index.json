{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Serverless Applications Serverless architecture is a way to build and run applications and services without managing infrastructure. Your application still runs on servers, but all the server management is done by the cloud providers, such as AWS, Azure, or GCP. What is AWS lambda? AWS Lambda is a serverless computing service that can run code without worrying about provisioning or managing servers. You only pay for what you use and are not charged anything if your application is not being used. It was originally developed to run computationally less expensive workloads, but now, it can also be used for larger applications. It can scale pretty well and supports up to thousands of concurrent requests. A simple hello world lambda application looks as follows: import json def lambda_handler ( event , context ): # TODO logic of the function return { 'statusCode' : 200 , 'body' : json . dumps ( 'Hello from Lambda!' ) } This function gets input data (in this example, no input is passed to the function) and returns a status code of 200 with a 'Hello from Lambda' message. Many AWS services can invoke the lambda function. If we want to access it from the internet, we need an API gateway to create an API in front of our Lambda function. What is new with AWS Lambda? Since December 2020, it has been possible now package and deploy Lambda functions as container images of up to 10 GB in storage size and six vCPUS. This means that large machine learning models can be deployed using a serverless approach that can be called in parallel to AWS lambda. In this repository, we deploy a BERT Question-Answering API in a serverless AWS Lambda environment. Therefore we use the Transformers library by HuggingFace, the Serverless Framework, AWS Lambda, and Amazon ECR. What is the AWS API gateway? Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs are the \"front door\" for applications to access data, business logic, or functionality from your backend services. API gateway for Lambda basically exposes the lambda function to the internet. In our machine learning examples, Lambda is the inference server, and the API gateway is where the data is passed to the Lambda function. The following figure shows the architecture of how the API gateway will interact with the python function that runs in Lambda. What is AWS DynamoDB? Amazon DynamoDB is a fully managed, serverless, NoSQL database designed to run high-performance applications at any scale. If we want to save some data during the transaction of data from and to the API gateway, we can use DynamoDB. In the case of machine learning examples, we will store the logs on our inference server. The primary information that we are interested in is the data that was sent to the ML inference server and the output of the ML model. We can use this to monitor and evaluate the performance of the model. The database can be attached to any visualization tool to see the real-time performance of the model. Project architecture The project's serverless backend uses AWS lambda, DynamoDB, API gateway, and ECR for container registry. The Lambda function is the inference server wrapped in a docker image and uploaded to the AWS ECR. AWS API gateway sends POST request payloads to the lambda function. AWS DynamoDB is used to store the data sent to the inference server for monitory purposes. Lambda can talk to DynamoDB using an IAM role that only allows writing requests to the database. How to use The Infrastructure is deployed using serverless library. Serverless is an easy to use and secure framework to build applications on AWS Lambda and other serverless functions by other cloud providers. Install serverless using npm if you don't have it already. If you don't have npm in your machine, follow the installation instructions in here npm install -g serverless If you want to start a project from zero, some templates can be used by running serverless in the terminal and follow the prompts. Here is an example of the serverless configuration file to deploy a Lambda function with API gateway: service : serverless-bert-qa-lambda-docker provider : name : aws # provider region : us-east-1 # aws region memorySize : 5120 # optional, in MB, default is 1024 timeout : 30 # optional, in seconds, default is 6 functions : questionanswering : image : ${ACOUNT_NUMBER}.dkr.ecr.us-east-1.amazonaws.com/bert-lambda:v1 #ecr url events : - http : path : qa # http path method : post # http method The complete serverless.yaml file that includes DynamoDB integration is written in this file . To deploy the architecture, just write the following in the same directory as the serverless.yaml file. shell script serverless deploy In the following few pages, we will show examples of using serverless architecture to deploy some of the commonly used NLP tasks. We will use state-of-the-art approaches using the transformers library.","title":"Serverless Applications"},{"location":"#serverless-applications","text":"Serverless architecture is a way to build and run applications and services without managing infrastructure. Your application still runs on servers, but all the server management is done by the cloud providers, such as AWS, Azure, or GCP.","title":"Serverless Applications"},{"location":"#what-is-aws-lambda","text":"AWS Lambda is a serverless computing service that can run code without worrying about provisioning or managing servers. You only pay for what you use and are not charged anything if your application is not being used. It was originally developed to run computationally less expensive workloads, but now, it can also be used for larger applications. It can scale pretty well and supports up to thousands of concurrent requests. A simple hello world lambda application looks as follows: import json def lambda_handler ( event , context ): # TODO logic of the function return { 'statusCode' : 200 , 'body' : json . dumps ( 'Hello from Lambda!' ) } This function gets input data (in this example, no input is passed to the function) and returns a status code of 200 with a 'Hello from Lambda' message. Many AWS services can invoke the lambda function. If we want to access it from the internet, we need an API gateway to create an API in front of our Lambda function.","title":"What is AWS lambda?"},{"location":"#what-is-new-with-aws-lambda","text":"Since December 2020, it has been possible now package and deploy Lambda functions as container images of up to 10 GB in storage size and six vCPUS. This means that large machine learning models can be deployed using a serverless approach that can be called in parallel to AWS lambda. In this repository, we deploy a BERT Question-Answering API in a serverless AWS Lambda environment. Therefore we use the Transformers library by HuggingFace, the Serverless Framework, AWS Lambda, and Amazon ECR.","title":"What is new with AWS Lambda?"},{"location":"#what-is-the-aws-api-gateway","text":"Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs are the \"front door\" for applications to access data, business logic, or functionality from your backend services. API gateway for Lambda basically exposes the lambda function to the internet. In our machine learning examples, Lambda is the inference server, and the API gateway is where the data is passed to the Lambda function. The following figure shows the architecture of how the API gateway will interact with the python function that runs in Lambda.","title":"What is the AWS API gateway?"},{"location":"#what-is-aws-dynamodb","text":"Amazon DynamoDB is a fully managed, serverless, NoSQL database designed to run high-performance applications at any scale. If we want to save some data during the transaction of data from and to the API gateway, we can use DynamoDB. In the case of machine learning examples, we will store the logs on our inference server. The primary information that we are interested in is the data that was sent to the ML inference server and the output of the ML model. We can use this to monitor and evaluate the performance of the model. The database can be attached to any visualization tool to see the real-time performance of the model.","title":"What is AWS DynamoDB?"},{"location":"#project-architecture","text":"The project's serverless backend uses AWS lambda, DynamoDB, API gateway, and ECR for container registry. The Lambda function is the inference server wrapped in a docker image and uploaded to the AWS ECR. AWS API gateway sends POST request payloads to the lambda function. AWS DynamoDB is used to store the data sent to the inference server for monitory purposes. Lambda can talk to DynamoDB using an IAM role that only allows writing requests to the database.","title":"Project architecture"},{"location":"#how-to-use","text":"The Infrastructure is deployed using serverless library. Serverless is an easy to use and secure framework to build applications on AWS Lambda and other serverless functions by other cloud providers. Install serverless using npm if you don't have it already. If you don't have npm in your machine, follow the installation instructions in here npm install -g serverless If you want to start a project from zero, some templates can be used by running serverless in the terminal and follow the prompts. Here is an example of the serverless configuration file to deploy a Lambda function with API gateway: service : serverless-bert-qa-lambda-docker provider : name : aws # provider region : us-east-1 # aws region memorySize : 5120 # optional, in MB, default is 1024 timeout : 30 # optional, in seconds, default is 6 functions : questionanswering : image : ${ACOUNT_NUMBER}.dkr.ecr.us-east-1.amazonaws.com/bert-lambda:v1 #ecr url events : - http : path : qa # http path method : post # http method The complete serverless.yaml file that includes DynamoDB integration is written in this file . To deploy the architecture, just write the following in the same directory as the serverless.yaml file. shell script serverless deploy In the following few pages, we will show examples of using serverless architecture to deploy some of the commonly used NLP tasks. We will use state-of-the-art approaches using the transformers library.","title":"How to use"},{"location":"question-ansering/","text":"Question Answering NLP Applications What is a Question and AnsWering NLP Have you ever wondered what happens when you ask a question on Google and immediately get an answer? For example, we ask `who is the president of the united states on the google search bar, and we get this answer: We can see that Google first searched and ranked 1.6 billion pages in 1.08 seconds and then performed an additional processing step to find the answer snippet from the page. The first task is the core product of Google as a search engine. The second processing technique (finding the answer to a question from a webpage) is the question and answering (QA) problem. Modern QA systems are semantic search engines, intelligent assistants, and automated information extractors. In the QA problems, a question and a context are passed to the model, returning the answer to the question in the context. This approach can be used to build an enterprise-level QA system. For example, a document search engine, such as Elastic Search, can be used to rank the documents, and a QA model to find the answer within the document. The Haystack library developed by Deepset, a German company focused on NLP can be used to build document ranking and answer questions within the document. Here is an example of a reader-retriever architecture for question-answering systems. In this document, we only focus on the QA answering part of the system, where we have a context and a question and want the answer. Model building We use the squad_v2 data set to fine-tune a pre-trained transformer model from the Hugging Face model hub. Squad_v2 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowd workers to look similar to answerable ones. The systems answer questions when possible and determine when the paragraph supports no answer and abstains from answering. We will use MobileBert, which is the compressed version of the popular BERT model. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE) 1 . We will bring the pre-trained model and tokenizer from the hugging face library. This python snipped, downloads the mibilbert model fine-tuned on Squad_v2 dataset into the ./model directory. from transformers import AutoModelForQuestionAnswering , AutoTokenizer def get_model ( model ): \"\"\"Loads model from Hugginface model hub into the ./model directory\"\"\" try : model = AutoModelForQuestionAnswering . from_pretrained ( model , use_cdn = True ) model . save_pretrained ( \"./model\" ) except Exception as e : raise ( e ) get_model ( \"mrm8488/mobilebert-uncased-finetuned-squadv2\" ) This python snniped, downloads the mibilbert tokenizer into the ./model directory. def get_tokenizer ( tokenizer ): \"\"\"Loads tokenizer from Hugginface model hub into the ./model directory\"\"\" try : tokenizer = AutoTokenizer . from_pretrained ( tokenizer ) tokenizer . save_pretrained ( \"./model\" ) except Exception as e : raise ( e ) get_tokenizer ( \"mrm8488/mobilebert-uncased-finetuned-squadv2\" ) Once we have the tokenizer, we will be able to encode the data that goes to the models and also decode the response from the model. This is the code snipped for the encoder function that takes the question, context, and tokenizer and returns attention_masks and the inpud_ids that will be passed to the model. def encode ( tokenizer , question , context ): \"\"\"encodes the question and context with a given tokenizer that is understandable to the model\"\"\" encoded = tokenizer . encode_plus ( question , context ) return encoded [ \"input_ids\" ], encoded [ \"attention_mask\" ] This code snipped will take the model's answer and decode it to a human-readable string format. def decode ( tokenizer , token ): \"\"\"decodes the tokens to the answer with a given tokenizer to return human readable response in a string format\"\"\" answer_tokens = tokenizer . convert_ids_to_tokens ( token , skip_special_tokens = True ) return tokenizer . convert_tokens_to_string ( answer_tokens ) We need to combine the encoder, model prediction, and decoder in a method. The following code snipped first loads the model and the tokenizer from the ./model directory. First, the question and context are passed through the encoder method, the output is passed through the model, and finally, the answer tokens are passed through the decode method to get the answer in a string format. from transformers import AutoModelForQuestionAnswering , AutoTokenizer , AutoConfig import torch def serverless_pipeline ( model_path = \"./model\" ): \"\"\"Initializes the model and tokenzier and returns a predict function that ca be used as pipeline\"\"\" tokenizer = AutoTokenizer . from_pretrained ( model_path ) model = AutoModelForQuestionAnswering . from_pretrained ( model_path ) def predict ( question , context ): \"\"\"predicts the answer on an given question and context. Uses encode and decode method from above\"\"\" input_ids , attention_mask = encode ( tokenizer , question , context ) start_scores , end_scores = model ( torch . tensor ([ input_ids ]), attention_mask = torch . tensor ([ attention_mask ]) ) ans_tokens = input_ids [ torch . argmax ( start_scores ) : torch . argmax ( end_scores ) + 1 ] answer = decode ( tokenizer , ans_tokens ) return answer return predict Storing the logs in DynamoDB DynamoDB is a fully managed, serverless, NoSQL database that is perfect for storing the model's inputs and outputs for monitoring and evaluating the model. We use the boto3 library to put the logs in our database. We keep the name of our DynamoDB in our Lambda's environment variable under DYNAMO_TABLE . We want to store the time, payload context and question, and the answer to the database. This code snipped shows how we keep the data in DyanmoDB after we know the answer to the question. import boto3 import os import uuid import time dynamodb = boto3 . resource ( \"dynamodb\" , region_name = \"us-east-1\" ) table = dynamodb . Table ( os . environ [ \"DYNAMODB_TABLE\" ]) timestamp = str ( time . time ()) item = { \"primary_key\" : str ( uuid . uuid1 ()), \"createdAt\" : timestamp , \"context\" : body [ \"context\" ], \"question\" : body [ \"question\" ], \"answer\" : answer , } table . put_item ( Item = item ) Lambda function Lambda is the serverless computing service from AWS, where the inference will be served. A Lambda handler is where the information from an API request passes through the function and returns the output to the API. We will include the DynamoDB writes code within the function as well. def handler ( event , context ): try : # loads the incoming event into a dictonary body = json . loads ( event [ \"body\" ]) # uses the pipeline to predict the answer answer = question_answering_pipeline ( question = body [ \"question\" ], context = body [ \"context\" ] ) timestamp = str ( time . time ()) item = { \"primary_key\" : str ( uuid . uuid1 ()), \"createdAt\" : timestamp , \"context\" : body [ \"context\" ], \"question\" : body [ \"question\" ], \"answer\" : answer , } table . put_item ( Item = item ) return { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" , \"Access-Control-Allow-Origin\" : \"*\" , \"Access-Control-Allow-Credentials\" : True , }, \"body\" : json . dumps ({ \"answer\" : answer }), } except Exception as e : print ( repr ( e )) return { \"statusCode\" : 500 , \"headers\" : { \"Content-Type\" : \"application/json\" , \"Access-Control-Allow-Origin\" : \"*\" , \"Access-Control-Allow-Credentials\" : True , }, \"body\" : json . dumps ({ \"error\" : repr ( e )}), } You can see all the code in handler.py file. Dockerize everything Since Lambda functions now support docker images, we can dockerize everything and upload it to the Amazon Elastic Container Registry (Amazon ECR) repository. The Lambda function will access this image to make predictions. The Dockerfile uses AWS's published base image for Lambda functions. FROM public.ecr.aws/lambda/python:3.8 # Copy function code and models into our /var/task COPY ./ ${ LAMBDA_TASK_ROOT } / # install our dependencies RUN python3 -m pip install -r requirements.txt --target ${ LAMBDA_TASK_ROOT } # run get_model.py to get model weights and tokenizers RUN python3 get_model.py # Set the CMD to your handler (could also be done as a parameter override outside of the Dockerfile) CMD [ \"handler.handler\" ] We need to send to build, tag and push the docker image to the ECR repository. First, we need to login to our ECR repository using aws CLI. aws_region = <your aws region> aws_account_id = <your aws account> aws ecr get-login-password --region $aws_region \\ | docker login username AWS --password-stdin $aws_account_id .dkr.ecr. $aws_region .amazonaws.com Then build, tag and push the docker image to the ECR repository. docker build -t nlp-lambda:v1 serverless-bert/. docker tag nlp-lambda:v1 $aws_account_id .dkr.ecr. $aws_region .amazonaws.com/nlp-lambda:v1 docker push $aws_account_id .dkr.ecr. $aws_region .amazonaws.com/nlp-lambda:v1 Serverless Question and Answering System","title":"Question Answering NLP Applications"},{"location":"question-ansering/#question-answering-nlp-applications","text":"","title":"Question Answering NLP Applications"},{"location":"question-ansering/#what-is-a-question-and-answering-nlp","text":"Have you ever wondered what happens when you ask a question on Google and immediately get an answer? For example, we ask `who is the president of the united states on the google search bar, and we get this answer: We can see that Google first searched and ranked 1.6 billion pages in 1.08 seconds and then performed an additional processing step to find the answer snippet from the page. The first task is the core product of Google as a search engine. The second processing technique (finding the answer to a question from a webpage) is the question and answering (QA) problem. Modern QA systems are semantic search engines, intelligent assistants, and automated information extractors. In the QA problems, a question and a context are passed to the model, returning the answer to the question in the context. This approach can be used to build an enterprise-level QA system. For example, a document search engine, such as Elastic Search, can be used to rank the documents, and a QA model to find the answer within the document. The Haystack library developed by Deepset, a German company focused on NLP can be used to build document ranking and answer questions within the document. Here is an example of a reader-retriever architecture for question-answering systems. In this document, we only focus on the QA answering part of the system, where we have a context and a question and want the answer.","title":"What is a Question and AnsWering NLP"},{"location":"question-ansering/#model-building","text":"We use the squad_v2 data set to fine-tune a pre-trained transformer model from the Hugging Face model hub. Squad_v2 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowd workers to look similar to answerable ones. The systems answer questions when possible and determine when the paragraph supports no answer and abstains from answering. We will use MobileBert, which is the compressed version of the popular BERT model. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE) 1 . We will bring the pre-trained model and tokenizer from the hugging face library. This python snipped, downloads the mibilbert model fine-tuned on Squad_v2 dataset into the ./model directory. from transformers import AutoModelForQuestionAnswering , AutoTokenizer def get_model ( model ): \"\"\"Loads model from Hugginface model hub into the ./model directory\"\"\" try : model = AutoModelForQuestionAnswering . from_pretrained ( model , use_cdn = True ) model . save_pretrained ( \"./model\" ) except Exception as e : raise ( e ) get_model ( \"mrm8488/mobilebert-uncased-finetuned-squadv2\" ) This python snniped, downloads the mibilbert tokenizer into the ./model directory. def get_tokenizer ( tokenizer ): \"\"\"Loads tokenizer from Hugginface model hub into the ./model directory\"\"\" try : tokenizer = AutoTokenizer . from_pretrained ( tokenizer ) tokenizer . save_pretrained ( \"./model\" ) except Exception as e : raise ( e ) get_tokenizer ( \"mrm8488/mobilebert-uncased-finetuned-squadv2\" ) Once we have the tokenizer, we will be able to encode the data that goes to the models and also decode the response from the model. This is the code snipped for the encoder function that takes the question, context, and tokenizer and returns attention_masks and the inpud_ids that will be passed to the model. def encode ( tokenizer , question , context ): \"\"\"encodes the question and context with a given tokenizer that is understandable to the model\"\"\" encoded = tokenizer . encode_plus ( question , context ) return encoded [ \"input_ids\" ], encoded [ \"attention_mask\" ] This code snipped will take the model's answer and decode it to a human-readable string format. def decode ( tokenizer , token ): \"\"\"decodes the tokens to the answer with a given tokenizer to return human readable response in a string format\"\"\" answer_tokens = tokenizer . convert_ids_to_tokens ( token , skip_special_tokens = True ) return tokenizer . convert_tokens_to_string ( answer_tokens ) We need to combine the encoder, model prediction, and decoder in a method. The following code snipped first loads the model and the tokenizer from the ./model directory. First, the question and context are passed through the encoder method, the output is passed through the model, and finally, the answer tokens are passed through the decode method to get the answer in a string format. from transformers import AutoModelForQuestionAnswering , AutoTokenizer , AutoConfig import torch def serverless_pipeline ( model_path = \"./model\" ): \"\"\"Initializes the model and tokenzier and returns a predict function that ca be used as pipeline\"\"\" tokenizer = AutoTokenizer . from_pretrained ( model_path ) model = AutoModelForQuestionAnswering . from_pretrained ( model_path ) def predict ( question , context ): \"\"\"predicts the answer on an given question and context. Uses encode and decode method from above\"\"\" input_ids , attention_mask = encode ( tokenizer , question , context ) start_scores , end_scores = model ( torch . tensor ([ input_ids ]), attention_mask = torch . tensor ([ attention_mask ]) ) ans_tokens = input_ids [ torch . argmax ( start_scores ) : torch . argmax ( end_scores ) + 1 ] answer = decode ( tokenizer , ans_tokens ) return answer return predict","title":"Model building"},{"location":"question-ansering/#storing-the-logs-in-dynamodb","text":"DynamoDB is a fully managed, serverless, NoSQL database that is perfect for storing the model's inputs and outputs for monitoring and evaluating the model. We use the boto3 library to put the logs in our database. We keep the name of our DynamoDB in our Lambda's environment variable under DYNAMO_TABLE . We want to store the time, payload context and question, and the answer to the database. This code snipped shows how we keep the data in DyanmoDB after we know the answer to the question. import boto3 import os import uuid import time dynamodb = boto3 . resource ( \"dynamodb\" , region_name = \"us-east-1\" ) table = dynamodb . Table ( os . environ [ \"DYNAMODB_TABLE\" ]) timestamp = str ( time . time ()) item = { \"primary_key\" : str ( uuid . uuid1 ()), \"createdAt\" : timestamp , \"context\" : body [ \"context\" ], \"question\" : body [ \"question\" ], \"answer\" : answer , } table . put_item ( Item = item )","title":"Storing the logs in DynamoDB"},{"location":"question-ansering/#lambda-function","text":"Lambda is the serverless computing service from AWS, where the inference will be served. A Lambda handler is where the information from an API request passes through the function and returns the output to the API. We will include the DynamoDB writes code within the function as well. def handler ( event , context ): try : # loads the incoming event into a dictonary body = json . loads ( event [ \"body\" ]) # uses the pipeline to predict the answer answer = question_answering_pipeline ( question = body [ \"question\" ], context = body [ \"context\" ] ) timestamp = str ( time . time ()) item = { \"primary_key\" : str ( uuid . uuid1 ()), \"createdAt\" : timestamp , \"context\" : body [ \"context\" ], \"question\" : body [ \"question\" ], \"answer\" : answer , } table . put_item ( Item = item ) return { \"statusCode\" : 200 , \"headers\" : { \"Content-Type\" : \"application/json\" , \"Access-Control-Allow-Origin\" : \"*\" , \"Access-Control-Allow-Credentials\" : True , }, \"body\" : json . dumps ({ \"answer\" : answer }), } except Exception as e : print ( repr ( e )) return { \"statusCode\" : 500 , \"headers\" : { \"Content-Type\" : \"application/json\" , \"Access-Control-Allow-Origin\" : \"*\" , \"Access-Control-Allow-Credentials\" : True , }, \"body\" : json . dumps ({ \"error\" : repr ( e )}), } You can see all the code in handler.py file.","title":"Lambda function"},{"location":"question-ansering/#dockerize-everything","text":"Since Lambda functions now support docker images, we can dockerize everything and upload it to the Amazon Elastic Container Registry (Amazon ECR) repository. The Lambda function will access this image to make predictions. The Dockerfile uses AWS's published base image for Lambda functions. FROM public.ecr.aws/lambda/python:3.8 # Copy function code and models into our /var/task COPY ./ ${ LAMBDA_TASK_ROOT } / # install our dependencies RUN python3 -m pip install -r requirements.txt --target ${ LAMBDA_TASK_ROOT } # run get_model.py to get model weights and tokenizers RUN python3 get_model.py # Set the CMD to your handler (could also be done as a parameter override outside of the Dockerfile) CMD [ \"handler.handler\" ] We need to send to build, tag and push the docker image to the ECR repository. First, we need to login to our ECR repository using aws CLI. aws_region = <your aws region> aws_account_id = <your aws account> aws ecr get-login-password --region $aws_region \\ | docker login username AWS --password-stdin $aws_account_id .dkr.ecr. $aws_region .amazonaws.com Then build, tag and push the docker image to the ECR repository. docker build -t nlp-lambda:v1 serverless-bert/. docker tag nlp-lambda:v1 $aws_account_id .dkr.ecr. $aws_region .amazonaws.com/nlp-lambda:v1 docker push $aws_account_id .dkr.ecr. $aws_region .amazonaws.com/nlp-lambda:v1","title":"Dockerize everything"},{"location":"question-ansering/#serverless","text":"Question and Answering System","title":"Serverless"},{"location":"transformer/","text":"NLP with Transformers Modern natural language processing (NLP) applications are typically built using transformer architectures proposed by Google researchers in 2017. 1 These architectures outperformed recurrent neural networks (RNN) and long short-term memory (LSTM) networks and also made transfer learning possible in the field of NLP. Two of the mosel popular transformer architectures that powers most of the NLP applications are the Generative Pretrained Transformer (GPT) 2 and Bidirectional Encoder Representations from Transformers (BERT). 3 Transfer Learning in NLP Transformers also made transfer learning possible in the field of NLP. Transfer learning is a very common practice in computer vision, where convolutional neural networks are trained on one task and then fine-tuned and adopted on a new task. Architecturally, this involves splitting the model into body and a head, where the head is a task-specific network. During training, the weights of the body learn broad features from large scale datasets such as ImageNet, and these weights are used to initialize a new model for the new task. This approach became a standard approach in computer vision. Most computer vision models in production are trained using transfer learning techniques. After the transformers architecture enabled the transformer learning in NLP, numerous institutions released their trained NLP models to be be used by the academics and practitioners. GPT and BERT are the two pre-trained models that sat a new state of the art across a variety of NLP benchmarks and ushered in the age of transformers. Overtime, different research institutions released different variation of the transformer architectures, some using PyTorch and other using Tensorflow, which made it hard for practitioners to use these models. HuggingFace created a set of unified APIs and a collection of pre-trained models and datasets that simplified the adoption of the state-of-teh art NLP models for the practitioners. HugginFace Library for NLP The HuggingFace Transformers provide a standardized interface to a wide range of transformer models as well as code and tools to adapt these models to new use cases. It also supports the three major deep learning frameworks: Pytorch, Tensorflow and JAX. The HuggingFace ecosystem consists of mainly two parts: a family of libraries and the Hub, as shown below. The libraries provide the code while the Hub provides the pretrained model weights, datasets, scripts for the evaluation metrics, and more.","title":"NLP with Transformers"},{"location":"transformer/#nlp-with-transformers","text":"Modern natural language processing (NLP) applications are typically built using transformer architectures proposed by Google researchers in 2017. 1 These architectures outperformed recurrent neural networks (RNN) and long short-term memory (LSTM) networks and also made transfer learning possible in the field of NLP. Two of the mosel popular transformer architectures that powers most of the NLP applications are the Generative Pretrained Transformer (GPT) 2 and Bidirectional Encoder Representations from Transformers (BERT). 3","title":"NLP with Transformers"},{"location":"transformer/#transfer-learning-in-nlp","text":"Transformers also made transfer learning possible in the field of NLP. Transfer learning is a very common practice in computer vision, where convolutional neural networks are trained on one task and then fine-tuned and adopted on a new task. Architecturally, this involves splitting the model into body and a head, where the head is a task-specific network. During training, the weights of the body learn broad features from large scale datasets such as ImageNet, and these weights are used to initialize a new model for the new task. This approach became a standard approach in computer vision. Most computer vision models in production are trained using transfer learning techniques. After the transformers architecture enabled the transformer learning in NLP, numerous institutions released their trained NLP models to be be used by the academics and practitioners. GPT and BERT are the two pre-trained models that sat a new state of the art across a variety of NLP benchmarks and ushered in the age of transformers. Overtime, different research institutions released different variation of the transformer architectures, some using PyTorch and other using Tensorflow, which made it hard for practitioners to use these models. HuggingFace created a set of unified APIs and a collection of pre-trained models and datasets that simplified the adoption of the state-of-teh art NLP models for the practitioners.","title":"Transfer Learning in NLP"},{"location":"transformer/#hugginface-library-for-nlp","text":"The HuggingFace Transformers provide a standardized interface to a wide range of transformer models as well as code and tools to adapt these models to new use cases. It also supports the three major deep learning frameworks: Pytorch, Tensorflow and JAX. The HuggingFace ecosystem consists of mainly two parts: a family of libraries and the Hub, as shown below. The libraries provide the code while the Hub provides the pretrained model weights, datasets, scripts for the evaluation metrics, and more.","title":"HugginFace Library for NLP"}]}